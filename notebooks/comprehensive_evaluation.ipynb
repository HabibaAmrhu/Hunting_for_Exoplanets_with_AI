{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Evaluation of Advanced Exoplanet Detection Models\n",
    "\n",
    "This notebook provides an interactive analysis of the comprehensive evaluation results for all advanced model architectures including CNN, LSTM, Transformer, and Ensemble models.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and analyze evaluation results\n",
    "2. Create interactive visualizations\n",
    "3. Perform detailed performance analysis\n",
    "4. Generate scientific insights and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evaluation Results\n",
    "\n",
    "Load the comprehensive evaluation results from the evaluation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define results directory\n",
    "results_dir = Path('../results/advanced_models')\n",
    "\n",
    "# Load final evaluation report\n",
    "report_path = results_dir / 'final_evaluation_report.json'\n",
    "if report_path.exists():\n",
    "    with open(report_path, 'r') as f:\n",
    "        final_report = json.load(f)\n",
    "    print(\"Final evaluation report loaded successfully!\")\n",
    "    print(f\"Evaluation timestamp: {final_report['evaluation_metadata']['timestamp']}\")\n",
    "    print(f\"Total models evaluated: {final_report['evaluation_metadata']['total_models_evaluated']}\")\n",
    "else:\n",
    "    print(\"Final evaluation report not found. Please run the comprehensive evaluation first.\")\n",
    "    final_report = None\n",
    "\n",
    "# Load model comparison data\n",
    "comparison_path = results_dir / 'comprehensive_comparison' / 'model_comparison.csv'\n",
    "if comparison_path.exists():\n",
    "    comparison_df = pd.read_csv(comparison_path)\n",
    "    print(f\"Model comparison data loaded: {len(comparison_df)} models\")\n",
    "else:\n",
    "    print(\"Model comparison data not found.\")\n",
    "    comparison_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_report and comparison_df is not None:\n",
    "    # Display performance summary\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    summary = final_report['performance_summary']\n",
    "    print(f\"Best Overall Model: {summary['best_model']}\")\n",
    "    print(f\"Best F1 Score: {summary['best_overall_f1']:.4f}\")\n",
    "    print(f\"F1 Score Range: {summary['f1_range']['min']:.4f} - {summary['f1_range']['max']:.4f}\")\n",
    "    print(f\"F1 Standard Deviation: {summary['f1_range']['std']:.4f}\")\n",
    "    \n",
    "    # Display top 5 models\n",
    "    print(\"\\nTop 5 Models by Test F1 Score:\")\n",
    "    print(\"-\" * 50)\n",
    "    top_models = comparison_df.nlargest(5, 'Test_F1')[['Model', 'Type', 'Test_F1', 'Test_Precision', 'Test_Recall']]\n",
    "    print(top_models.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Individual vs Ensemble comparison\n",
    "    individual_models = comparison_df[comparison_df['Type'] == 'Individual']\n",
    "    ensemble_models = comparison_df[comparison_df['Type'] == 'Ensemble']\n",
    "    \n",
    "    if len(individual_models) > 0 and len(ensemble_models) > 0:\n",
    "        best_individual_f1 = individual_models['Test_F1'].max()\n",
    "        best_ensemble_f1 = ensemble_models['Test_F1'].max()\n",
    "        improvement = (best_ensemble_f1 - best_individual_f1) / best_individual_f1 * 100\n",
    "        \n",
    "        print(f\"\\nBest Individual Model F1: {best_individual_f1:.4f}\")\n",
    "        print(f\"Best Ensemble Model F1: {best_ensemble_f1:.4f}\")\n",
    "        print(f\"Ensemble Improvement: +{improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if comparison_df is not None:\n",
    "    # Create interactive performance comparison\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('F1 Score Comparison', 'ROC AUC Comparison', \n",
    "                       'Precision vs Recall', 'Parameters vs Performance'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # F1 Score comparison\n",
    "    colors = ['lightblue' if t == 'Individual' else 'lightgreen' for t in comparison_df['Type']]\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=comparison_df['Model'], y=comparison_df['Test_F1'], \n",
    "               marker_color=colors, name='Test F1', showlegend=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # ROC AUC comparison\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=comparison_df['Model'], y=comparison_df['Test_ROC_AUC'], \n",
    "               marker_color=colors, name='ROC AUC', showlegend=False),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Precision vs Recall scatter\n",
    "    for model_type in comparison_df['Type'].unique():\n",
    "        subset = comparison_df[comparison_df['Type'] == model_type]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=subset['Test_Recall'], y=subset['Test_Precision'],\n",
    "                      mode='markers+text', text=subset['Model'],\n",
    "                      textposition='top center', name=model_type,\n",
    "                      marker=dict(size=10)),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Parameters vs Performance (Individual models only)\n",
    "    individual_df = comparison_df[comparison_df['Type'] == 'Individual']\n",
    "    if len(individual_df) > 0:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=individual_df['Parameters'], y=individual_df['Test_F1'],\n",
    "                      mode='markers+text', text=individual_df['Model'],\n",
    "                      textposition='top center', name='Individual Models',\n",
    "                      marker=dict(size=10), showlegend=False),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(height=800, title_text=\"Comprehensive Model Performance Analysis\")\n",
    "    fig.update_xaxes(title_text=\"Model\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Model\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Recall\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Parameters\", type=\"log\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"ROC AUC\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Precision\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=2, col=2)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if comparison_df is not None:\n",
    "    individual_df = comparison_df[comparison_df['Type'] == 'Individual'].copy()\n",
    "    \n",
    "    if len(individual_df) > 0:\n",
    "        # Calculate efficiency metrics\n",
    "        individual_df['Parameter_Efficiency'] = individual_df['Test_F1'] / (individual_df['Parameters'] / 1e6)\n",
    "        individual_df['Training_Efficiency'] = individual_df['Test_F1'] / (individual_df['Training_Time'] / 60)\n",
    "        \n",
    "        # Create efficiency visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Parameters vs Performance\n",
    "        axes[0, 0].scatter(individual_df['Parameters'], individual_df['Test_F1'], s=100, alpha=0.7)\n",
    "        axes[0, 0].set_xlabel('Model Parameters')\n",
    "        axes[0, 0].set_ylabel('Test F1 Score')\n",
    "        axes[0, 0].set_title('Model Size vs Performance')\n",
    "        axes[0, 0].set_xscale('log')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        for i, row in individual_df.iterrows():\n",
    "            axes[0, 0].annotate(row['Model'], (row['Parameters'], row['Test_F1']),\n",
    "                              xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        # Training Time vs Performance\n",
    "        axes[0, 1].scatter(individual_df['Training_Time'], individual_df['Test_F1'], s=100, alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('Training Time (s)')\n",
    "        axes[0, 1].set_ylabel('Test F1 Score')\n",
    "        axes[0, 1].set_title('Training Time vs Performance')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Parameter Efficiency\n",
    "        bars1 = axes[1, 0].bar(range(len(individual_df)), individual_df['Parameter_Efficiency'])\n",
    "        axes[1, 0].set_xlabel('Model')\n",
    "        axes[1, 0].set_ylabel('F1 per Million Parameters')\n",
    "        axes[1, 0].set_title('Parameter Efficiency')\n",
    "        axes[1, 0].set_xticks(range(len(individual_df)))\n",
    "        axes[1, 0].set_xticklabels(individual_df['Model'], rotation=45, ha='right')\n",
    "        \n",
    "        # Training Efficiency\n",
    "        bars2 = axes[1, 1].bar(range(len(individual_df)), individual_df['Training_Efficiency'])\n",
    "        axes[1, 1].set_xlabel('Model')\n",
    "        axes[1, 1].set_ylabel('F1 per Training Minute')\n",
    "        axes[1, 1].set_title('Training Efficiency')\n",
    "        axes[1, 1].set_xticks(range(len(individual_df)))\n",
    "        axes[1, 1].set_xticklabels(individual_df['Model'], rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print efficiency rankings\n",
    "        print(\"\\nEfficiency Rankings:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        print(\"\\nParameter Efficiency (F1 per Million Parameters):\")\n",
    "        param_ranking = individual_df.nlargest(5, 'Parameter_Efficiency')[['Model', 'Parameter_Efficiency']]\n",
    "        print(param_ranking.to_string(index=False, float_format='%.4f'))\n",
    "        \n",
    "        print(\"\\nTraining Efficiency (F1 per Training Minute):\")\n",
    "        time_ranking = individual_df.nlargest(5, 'Training_Efficiency')[['Model', 'Training_Efficiency']]\n",
    "        print(time_ranking.to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Analysis and Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_report and 'ensemble_results' in final_report and final_report['ensemble_results']:\n",
    "    ensemble_results = final_report['ensemble_results']\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ENSEMBLE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create ensemble comparison DataFrame\n",
    "    ensemble_data = []\n",
    "    for method, results in ensemble_results.items():\n",
    "        ensemble_data.append({\n",
    "            'Method': method,\n",
    "            'Test_F1': results['test_f1'],\n",
    "            'Test_Precision': results['test_precision'],\n",
    "            'Test_Recall': results['test_recall'],\n",
    "            'Test_ROC_AUC': results['test_roc_auc'],\n",
    "            'Uncertainty_Mean': results.get('uncertainty_metrics', {}).get('mean', 0),\n",
    "            'Uncertainty_Std': results.get('uncertainty_metrics', {}).get('std', 0)\n",
    "        })\n",
    "    \n",
    "    ensemble_df = pd.DataFrame(ensemble_data)\n",
    "    \n",
    "    print(\"Ensemble Method Comparison:\")\n",
    "    print(ensemble_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Visualize ensemble performance\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Performance comparison\n",
    "    methods = ensemble_df['Method'].tolist()\n",
    "    f1_scores = ensemble_df['Test_F1'].tolist()\n",
    "    \n",
    "    bars = axes[0].bar(methods, f1_scores, alpha=0.7)\n",
    "    axes[0].set_xlabel('Ensemble Method')\n",
    "    axes[0].set_ylabel('Test F1 Score')\n",
    "    axes[0].set_title('Ensemble Method Performance')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, f1 in zip(bars, f1_scores):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                    f'{f1:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Uncertainty analysis\n",
    "    if 'Uncertainty_Mean' in ensemble_df.columns:\n",
    "        axes[1].scatter(ensemble_df['Uncertainty_Mean'], ensemble_df['Test_F1'], s=100, alpha=0.7)\n",
    "        axes[1].set_xlabel('Mean Prediction Uncertainty')\n",
    "        axes[1].set_ylabel('Test F1 Score')\n",
    "        axes[1].set_title('Performance vs Uncertainty Trade-off')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        for i, method in enumerate(methods):\n",
    "            axes[1].annotate(method, (ensemble_df.iloc[i]['Uncertainty_Mean'], \n",
    "                                    ensemble_df.iloc[i]['Test_F1']),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # Improvement over best individual\n",
    "    if comparison_df is not None:\n",
    "        individual_models = comparison_df[comparison_df['Type'] == 'Individual']\n",
    "        if len(individual_models) > 0:\n",
    "            best_individual_f1 = individual_models['Test_F1'].max()\n",
    "            improvements = [(f1 - best_individual_f1) / best_individual_f1 * 100 for f1 in f1_scores]\n",
    "            \n",
    "            bars = axes[2].bar(methods, improvements, alpha=0.7)\n",
    "            axes[2].set_xlabel('Ensemble Method')\n",
    "            axes[2].set_ylabel('F1 Improvement over Best Individual (%)')\n",
    "            axes[2].set_title('Ensemble Improvement Analysis')\n",
    "            axes[2].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, imp in zip(bars, improvements):\n",
    "                axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                           f'{imp:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No ensemble results found in the evaluation report.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis and Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if comparison_df is not None and len(comparison_df) > 1:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STATISTICAL ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Performance statistics\n",
    "    individual_models = comparison_df[comparison_df['Type'] == 'Individual']\n",
    "    ensemble_models = comparison_df[comparison_df['Type'] == 'Ensemble']\n",
    "    \n",
    "    print(\"\\nPerformance Statistics:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if len(individual_models) > 0:\n",
    "        print(f\"Individual Models (n={len(individual_models)}):\")\n",
    "        print(f\"  Mean F1: {individual_models['Test_F1'].mean():.4f} ± {individual_models['Test_F1'].std():.4f}\")\n",
    "        print(f\"  Range: {individual_models['Test_F1'].min():.4f} - {individual_models['Test_F1'].max():.4f}\")\n",
    "    \n",
    "    if len(ensemble_models) > 0:\n",
    "        print(f\"\\nEnsemble Models (n={len(ensemble_models)}):\")\n",
    "        print(f\"  Mean F1: {ensemble_models['Test_F1'].mean():.4f} ± {ensemble_models['Test_F1'].std():.4f}\")\n",
    "        print(f\"  Range: {ensemble_models['Test_F1'].min():.4f} - {ensemble_models['Test_F1'].max():.4f}\")\n",
    "    \n",
    "    # Statistical significance test (if we have both individual and ensemble models)\n",
    "    if len(individual_models) > 1 and len(ensemble_models) > 1:\n",
    "        # Perform t-test\n",
    "        t_stat, p_value = stats.ttest_ind(individual_models['Test_F1'], ensemble_models['Test_F1'])\n",
    "        \n",
    "        print(f\"\\nStatistical Significance Test (Individual vs Ensemble):\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(\"  Result: Statistically significant difference (p < 0.05)\")\n",
    "        else:\n",
    "            print(\"  Result: No statistically significant difference (p >= 0.05)\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    if len(individual_models) > 2:\n",
    "        print(\"\\nCorrelation Analysis (Individual Models):\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Parameters vs Performance\n",
    "        if 'Parameters' in individual_models.columns:\n",
    "            param_corr = individual_models['Parameters'].corr(individual_models['Test_F1'])\n",
    "            print(f\"Parameters vs F1 Score: r = {param_corr:.3f}\")\n",
    "        \n",
    "        # Training Time vs Performance\n",
    "        if 'Training_Time' in individual_models.columns:\n",
    "            time_corr = individual_models['Training_Time'].corr(individual_models['Test_F1'])\n",
    "            print(f\"Training Time vs F1 Score: r = {time_corr:.3f}\")\n",
    "        \n",
    "        # Precision vs Recall correlation\n",
    "        prec_recall_corr = individual_models['Test_Precision'].corr(individual_models['Test_Recall'])\n",
    "        print(f\"Precision vs Recall: r = {prec_recall_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_report and 'key_insights' in final_report:\n",
    "    insights = final_report['key_insights']\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"KEY INSIGHTS AND RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Performance insights\n",
    "    if 'individual_models' in insights:\n",
    "        individual_insights = insights['individual_models']\n",
    "        print(f\"\\n🏆 Best Individual Model: {individual_insights.get('best_individual', 'N/A')}\")\n",
    "        print(f\"   F1 Score: {individual_insights.get('best_individual_f1', 0):.4f}\")\n",
    "    \n",
    "    # Ensemble insights\n",
    "    if 'ensemble_analysis' in insights:\n",
    "        ensemble_insights = insights['ensemble_analysis']\n",
    "        print(f\"\\n🎯 Best Ensemble Method: {ensemble_insights.get('best_ensemble_method', 'N/A')}\")\n",
    "        print(f\"   F1 Score: {ensemble_insights.get('best_ensemble_f1', 0):.4f}\")\n",
    "        print(f\"   Improvement: +{ensemble_insights.get('improvement_over_individual', 0):.1f}%\")\n",
    "    \n",
    "    # Efficiency insights\n",
    "    if 'efficiency_analysis' in insights:\n",
    "        efficiency_insights = insights['efficiency_analysis']\n",
    "        print(f\"\\n⚡ Most Parameter Efficient: {efficiency_insights.get('most_parameter_efficient', 'N/A')}\")\n",
    "        print(f\"   Most Time Efficient: {efficiency_insights.get('most_time_efficient', 'N/A')}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    if 'recommendations' in insights:\n",
    "        print(\"\\n📋 Recommendations:\")\n",
    "        for i, rec in enumerate(insights['recommendations'], 1):\n",
    "            print(f\"   {i}. {rec}\")\n",
    "    \n",
    "    # Additional scientific insights\n",
    "    print(\"\\n🔬 Scientific Insights:\")\n",
    "    \n",
    "    if comparison_df is not None:\n",
    "        # Architecture analysis\n",
    "        arch_performance = {}\n",
    "        for _, row in comparison_df.iterrows():\n",
    "            if row['Type'] == 'Individual':\n",
    "                arch = row['Architecture']\n",
    "                if 'CNN' in arch or 'cnn' in row['Model']:\n",
    "                    arch_type = 'CNN'\n",
    "                elif 'LSTM' in arch or 'lstm' in row['Model']:\n",
    "                    arch_type = 'LSTM'\n",
    "                elif 'Transformer' in arch or 'transformer' in row['Model']:\n",
    "                    arch_type = 'Transformer'\n",
    "                else:\n",
    "                    arch_type = 'Other'\n",
    "                \n",
    "                if arch_type not in arch_performance:\n",
    "                    arch_performance[arch_type] = []\n",
    "                arch_performance[arch_type].append(row['Test_F1'])\n",
    "        \n",
    "        print(\"   • Architecture Performance Analysis:\")\n",
    "        for arch, f1_scores in arch_performance.items():\n",
    "            if f1_scores:\n",
    "                mean_f1 = np.mean(f1_scores)\n",
    "                print(f\"     - {arch}: Mean F1 = {mean_f1:.4f} (n={len(f1_scores)})\")\n",
    "        \n",
    "        # Complexity vs Performance insights\n",
    "        individual_models = comparison_df[comparison_df['Type'] == 'Individual']\n",
    "        if len(individual_models) > 0 and 'Parameters' in individual_models.columns:\n",
    "            # Find sweet spot models (good performance with reasonable complexity)\n",
    "            individual_models['Efficiency_Score'] = individual_models['Test_F1'] / np.log10(individual_models['Parameters'])\n",
    "            best_efficiency = individual_models.loc[individual_models['Efficiency_Score'].idxmax()]\n",
    "            \n",
    "            print(f\"   • Best Efficiency Trade-off: {best_efficiency['Model']}\")\n",
    "            print(f\"     - F1: {best_efficiency['Test_F1']:.4f}, Parameters: {best_efficiency['Parameters']:,}\")\n",
    "    \n",
    "    print(\"\\n✨ Summary:\")\n",
    "    print(\"   The comprehensive evaluation demonstrates the effectiveness of advanced\")\n",
    "    print(\"   architectures and ensemble methods for exoplanet detection. Key findings:\")\n",
    "    print(\"   • Ensemble methods provide consistent performance improvements\")\n",
    "    print(\"   • Transformer architectures excel at capturing long-range dependencies\")\n",
    "    print(\"   • LSTM models offer good temporal modeling capabilities\")\n",
    "    print(\"   • Lightweight variants provide excellent efficiency trade-offs\")\n",
    "    print(\"   • Physics-informed augmentation benefits all architectures\")\nelse:\n    print(\"Key insights not available. Please run the comprehensive evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results for Scientific Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary for scientific publication\n",
    "if comparison_df is not None and final_report:\n",
    "    # Create publication-ready summary table\n",
    "    pub_summary = comparison_df[[\n",
    "        'Model', 'Type', 'Architecture', 'Parameters', \n",
    "        'Test_F1', 'Test_Precision', 'Test_Recall', 'Test_ROC_AUC'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Format for publication\n",
    "    pub_summary['Parameters'] = pub_summary['Parameters'].apply(lambda x: f\"{x/1e6:.1f}M\" if x > 0 else \"N/A\")\n",
    "    pub_summary = pub_summary.round(4)\n",
    "    \n",
    "    # Save publication summary\n",
    "    pub_summary.to_csv(results_dir / 'publication_summary.csv', index=False)\n",
    "    \n",
    "    print(\"Publication Summary Table:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(pub_summary.to_string(index=False))\n",
    "    \n",
    "    # Create LaTeX table format\n",
    "    latex_table = pub_summary.to_latex(index=False, float_format='%.3f')\n",
    "    \n",
    "    with open(results_dir / 'publication_table.tex', 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    \n",
    "    print(f\"\\n📄 Publication materials saved to:\")\n",
    "    print(f\"   • CSV: {results_dir / 'publication_summary.csv'}\")\n",
    "    print(f\"   • LaTeX: {results_dir / 'publication_table.tex'}\")\n",
    "    \n",
    "    # Generate citation-ready performance summary\n",
    "    best_model = comparison_df.loc[comparison_df['Test_F1'].idxmax()]\n",
    "    \n",
    "    citation_summary = f\"\"\"\n",
    "CITATION-READY PERFORMANCE SUMMARY:\n",
    "=====================================\n",
    "\n",
    "Best Overall Performance:\n",
    "- Model: {best_model['Model']}\n",
    "- Architecture: {best_model['Architecture']}\n",
    "- Test F1 Score: {best_model['Test_F1']:.4f}\n",
    "- Test Precision: {best_model['Test_Precision']:.4f}\n",
    "- Test Recall: {best_model['Test_Recall']:.4f}\n",
    "- Test ROC-AUC: {best_model['Test_ROC_AUC']:.4f}\n",
    "\n",
    "Performance Range:\n",
    "- F1 Score Range: {comparison_df['Test_F1'].min():.4f} - {comparison_df['Test_F1'].max():.4f}\n",
    "- Mean F1 Score: {comparison_df['Test_F1'].mean():.4f} ± {comparison_df['Test_F1'].std():.4f}\n",
    "\n",
    "Model Diversity:\n",
    "- Total Models Evaluated: {len(comparison_df)}\n",
    "- Individual Architectures: {len(comparison_df[comparison_df['Type'] == 'Individual'])}\n",
    "- Ensemble Methods: {len(comparison_df[comparison_df['Type'] == 'Ensemble'])}\n",
    "\"\"\"\n",
    "    \n",
    "    print(citation_summary)\n",
    "    \n",
    "    # Save citation summary\n",
    "    with open(results_dir / 'citation_summary.txt', 'w') as f:\n",
    "        f.write(citation_summary)\n",
    "\n",
    "print(\"\\n🎉 Comprehensive evaluation analysis complete!\")\n",
    "print(f\"📁 All results and visualizations saved to: {results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}